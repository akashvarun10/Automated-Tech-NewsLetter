{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import YoutubeLoader\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains.summarize import load_summarize_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "import os\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "api_key = os.getenv(\"OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/langchain/lib/python3.9/site-packages/langchain_community/llms/openai.py:249: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain_community.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n",
      "/opt/homebrew/Caskroom/miniforge/base/envs/langchain/lib/python3.9/site-packages/langchain_community/llms/openai.py:1070: UserWarning: You are trying to use a chat model. This way of initializing it is no longer supported. Instead, please use: `from langchain_community.chat_models import ChatOpenAI`\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "llm = OpenAI(temperature=0,model_name=\"gpt-3.5-turbo\", api_key=api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube_url_list = [\"https://youtu.be/_j7JEDWuqLE?si=BY_5dZNxCfNdYC4S\",\"https://youtu.be/KIvl-VY8H0Y?si=MQqh5_1v-ut9E7AO\"]\n",
    "\n",
    "texts = []\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=2000, chunk_overlap=0)\n",
    "\n",
    "for url in youtube_url_list:\n",
    "    loader = YoutubeLoader.from_youtube_url(url, add_video_info=True)\n",
    "    result = loader.load()\n",
    "    \n",
    "\n",
    "    texts.extend(text_splitter.split_documents(result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/langchain/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:117: LangChainDeprecationWarning: The function `run` was deprecated in LangChain 0.1.0 and will be removed in 0.2.0. Use invoke instead.\n",
      "  warn_deprecated(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new MapReduceDocumentsChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"if you are building AI apps you have to learn how to use hugging face it is one of the top AI companies valued more than 2 billion dollars it has more than 16 000 followers on GitHub its product is used by Google Amazon Microsoft and meta with more than 200 000 different type of AI models including image to text text to speech below text to image and many more that's why if you are building AI apps you absolutely need to learn how to use it and I'm going to show you how can you use hugging face platform and build it with other Public Library like launching let's get to it in the show hugging face is a place for you to discover and share AI models so there are three parts of the hugging phase platform models datasets and space firstly is models this place where you can find all different sorts of models to use for example if we are interested in using image text I can select the category on the left and then on the right side it choose any of the popular image to text model and once I get into this page I on the left side they will have some description about model and on the right side it allows you to preview and test the AI model directly on their hosted version and this is why Hackney face is so useful so without it you will need to find the models download it to your local machine or host somewhere and then try to run it to know if it is the right model for you but with hugging face they are hosting on their own machine and you can test it immediately but for this image to text model I can drag and drop image directly and see what kind of results it will get if and want to use it it allows you to easily deploy this model on different servers you can also use host API on hugging face hub for free it is a bit slow and they have rate limits but it's definitely enough for you to run some tests but on the other hand if you prefer to run the models locally on your own machine you can also use their Transformers library and I will talk about how do we do that very soon\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"the listen model on the outside they also have data sets and this is where you can find a lot of data sets that you can use to train your own model for example if I want to build my own voice model I can filter down to text to speech and find a specific language that I want to use then you can click on any of them and preview what are the data sets they have unless you are training your own model you probably won't use this data sets too much and the last part is space space is initially designed for people to Showcase and share the AI apps that they build so they allow you to deploy the apps that you have been building very easily on their own machine and they provide free version too but on the other side you can explore what other AI apps that people are building and there are a lot of very cool stuff you can just click on them and start playing with those apps and you can also learn how do they build those things clicking on this button it will show you all the models are used to build these apps and you can click on the files to say the source code how are we going to use those models on hugging face while we are implementing Inland chain I'll take you through a step-by-step example of implementing such AI app where I can upload the image and then it can automatically turn it into an audio story the man and woman sat on the couch lost in silence he broke it I love you she smiled and said I know as through this example you will learn how to use a few different hugging face AI models let's get to it firstly let's think step by step how we're going to implement this this app will have three components where first they need an image to text model to let the machine understand what is the scenario based on the photo and then we will use large language model to generate a short story and in the end we'll use a text to speech model to generate the audio story and to find the right image to text model we can go to hugging face and filter down the image text models the\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"one I will be using is the one called blip you will need to create a hugging face account and then go to settings access tokens and create an access token for land chain let's go back to the visual studio and create a DOT NV file where we restore all the credentials an output hugging face Hub API token once I save that let's import a few libraries we'll reverse the import.env and run this to be able to access hugging face API token that we store in the EMV file and then we will import pipelines from Transformers so pipeline will allow us to download the hugging phase model into our local machine and now we're ready to implement the first one image into text model we've created pipeline to load the AI model firstly we were putting the task here which is image of the text some of you might be curious where do we get this task name image to text from so hugging phase Transformers Library actually have a predefined list of tasks and you can go to this URL huggingface.com tasks to understand what are the tasks that it supports and you can click on any of them to get a more detailed tutorial about how to use that specific tasks and then we need to put the model name you can get a model Name by clicking on this using Transformers and just copy paste this one and we will run image to text pass on the URL of the image file then we're going to print it in our copy paste a photo in the root folder for the testing purpose now let's see what results we got we run python app.py now gather these results a group of people standing on a boat which is very accurate description and I only want to return the actual text here so I will do choose the library of the first item and generated tax and next is that we want to use a large language model to generate a short story based on this scenario that we got from the image you can use some open source model hugging face as well but for me I do prefer use GPT so use launching here so that's adding the open AI API key here as well and then\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"let's import a few libraries from Land chain and this is a function we're going to run we will firstly create a prompt template that lets GPT to generate story and then we'll create this lrm chain with GPD 3.5 turbo let's try and now all we need to do is just turn those tags into a speech using a text to speech model and again we will do the same thing go to the models page find text-to-speech models and find the most popular one but this time I want to share another way you can use hugging face model so you can click on this deploy button and there should be option called info difference API and this is super easy and fast way for you to test out the honeyface API for free so this is what we're gonna do we're gonna use this ring request Library going back here import requests and then create a function text to speech and at the top I'm going to load the hugging face API token so that I can pass on to the API request and I were adding the API URL here putting the header that passed on my hugging face API token and then create a message of inputs now just call this API requests and for the model I'm using the result it returns is a Flac file which is one type of audio file so I'm going to store it locally let's try it oh sorry I forgot to add the OS Library which allow us to get the API token and let me try again okay here we go we got this audio file generated the group of people were standing on the boat their eyes fixed on the horizon the sample setting painting the sky with brilliant shades of pink pink and orange okay now you can see whole thing has been working all we need to do now just connecting everything together and give the UI a layer with streamlit so our import streamlit as ST which is the library that allow us create a user interface for python code and button I will add this and also create a main function which will be called when the app is loaded so first I will reset the page title and then it will give header turn image into audio story I will\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"put a uploaded file equal to St dot file uploader this will allow people to upload a image file then if the file is uploaded I will firstly try to save this image and display the image by sd.image then I will call in the function that we created get the model to generate text from the image we uploaded and then let GPT generate a story based on the scenario in the end our general audio file from the stories and we're going to display scenario and stories in here and in the end we're gonna display the audio file that we got and that's pretty much it let's try to run this app by doing streamlit round app.p1 okay so here we can upload the image and we can see it is running here if we open the terminal we should be able to see what it's doing now okay here you go you can see we already generate scenario and little story here if you click this play button it should play the audio file as they said together on the couch the man stared intently at the woman he had known for 40 years and every time he looked at her he felt like he was seeing her for the first time suddenly he blurted I think I love you the woman turned to him her eyes wide with surprise and then with a smile that lit up the room she said finally from that moment on they knew they were meant to be together okay I think of this pretty dope use case so this is how you can use hugging face models he quickly recap the easiest way for you to use that would be you can use inference API to use their hosted version directly on the other side you can also use pipeline to download all those models on your local machine if you want to learn more I highly recommend you go to thehf.com tasks to learn all the different type of tasks if it supports as well as different type of models it has one last thing I want to touch base on is I realized one of the low code AI chin build a platform called relevance AI they actually provide image to hacks model out of box and I can just create this image to speech app super quickly\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"with their local UI and got it Droid app out of box in just like five minutes I do hope they can build a deep integration with hugging face where I can just grab different type of AI models directly but it's already a pretty good start so highly recommend alright hopefully you know how to use hacking face now and start building some super interesting AI apps if you found this content useful Please Subscribe or continue sharing all those AI experiments I'm doing thank you and see you next time\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"autogen from Microsoft now has a pretty nice user interface called autogen Studio you can use this user interface to create skills agents and workflows for building multi-agent capabilities autogen studio is op source and you can run this locally on your own machine and that's exactly what we're going to be doing today in this video I'll show you how to set this up locally on your own machine and then we will look at a few examples in order to install autogen on your local machine you have two options first is to install the python package via pip and the second option is to build it from Source in this video we're going to go with the easier option so we will install the python package for that first we need to create a condo virtual environment I'm calling the virtual environment UT to gen and we're going to be using python 3.10 first just create the virtual in environment then we need to activate the virtual environment using cond activate and provide the virtual environment name you can see that the virtu environment has been activated now we need to set up our openi API key so here I'm providing my open a API key I will reok this after this video you can also use open source LMS with autogen Studio I'll show you that in another video next we need to install the autogen studio package so we're going to use pip install autogen Studio that will install the package for us you can see that it is already installed in my local machine next in order to run the autogen studio we're going to use the autogen studio command with UI and we will need to run this on a specific port in this case we are running this on 8081 but you can use any port that you want when we run this this will show us a URL this is running on Local Host Port 8081 you need to copy this and paste it in your browser and here we have our autogen studio running there are three high level sections the first one is called build the second one is playground and the third one is called Gallery let's go with\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"them one by one the first section is build that focuses on defining the properties of agents and agent workflows this uh section contains the following three concepts the first one is skills second is agents and the third one is workflows let's look at them one by one the first one is skills these are basically python functions that describe how the agent can solve a given task so here are a few examples they are basically python functions now each skill is supposed to have a name of the function then a descriptive doc string which describes how exactly it's supposed to perform a specific task and this description is going to be used by the agents when they are introduced as part of the workflows at the inference time all these skills are going to be available to the agents you can also create your own new skills so click on new skill then provide the name of the skill that you want and here you need to replace this code uh with your own implementation the second concept is agents this provides an interface to declaratively specify property of autogen Agents so it's very similar to what you probably have seen in their python code now each agent is supposed to have a name so you can provide a name here then description of the agent what exactly this agent is supposed to do you also need to provide a system message which will control the behavior of a given agent after that you can specify which different models or llms this agent can use so here the default list has GPT 4 GPT 3.5 turbo and some open source models as well you can add and remove models here and the last thing is that you need to specify which skills are available to this model so for the time being the find paper from archive X as well as generate images these are the two skills that are defined by default and you can also add new skills if you want so in this case I'm going to just use the gp4 preview so I will remove these other two models and then click okay so by default there are two agents one\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"is the primary assistant the other one is user proxy so user proxy agent that execute codes if you look at it this one doesn't really have any system message but you can add one here and you can also add skills now similar to skills you can also Define your own agents and you'll need to provide the name description whether it can accept human input or not and the most important part is system message plus the skills that you want the agent to have in another video we're going to look at how to build example agents the last section is workflows an agent workflow is a specification of a set of agents that can work together to accomplish a task so here they have provided three different example workflows let's look at this General agent workflow now if you click on it first you need to provide the name of the workflow then the description of what the workflow is supposed to do so this is used for general purpose tasks then the uh next section is summary method so Define the method to summarize the conversation in agentic Frameworks like autogen multiple agents are interacting with each other to accomplish a task right so you can summarize their conversation for example this is just the last conversation if you don't want any summary you set it to none or if you want the llm to generate summary of all the conversation you can send that as well now in this case the sender is going to be the user proxy agent that simply execute code and the receiver is the primary assistant that we Define in here you can add more agents in here depending on the complexity of the task that you want to achieve just like skills and agents you can also create your work workflows here the next section is playground so here you can test different workflows that you have created so for example we can create a new session let's click on this next you need to select the workflow that you want to use so we will just use the general agent workflow click create and now you can start interacting with\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"this workflow if you go back to skills so we had the generate images skill and find papers from archive so let's test this agent my message is going to be create an image of an astronaut on a horse and let's see what happens okay pretty cool we have an image of an astronaut on a horse and I think it was using Del to create these images now let's look at what exactly happened in here so here is the python code that it created to execute our request so first it created uh a prompt or description based on our initial prompt and then it used the generate image skill to generate the image and save it you can see the actual interactions between the agents so there are a total of four different messages that were exchanged here's the initial prompt so this is basically the user proxy agent that accepts the user prompt then the primary assistant agent in this case says to create an image of an astronaut on a horse I'll use the generate and save images function from the skills. py file this function will generate the image based on the provided description and save it to disk let's proceed proed with generating the image and here's the python code it used here's the file name for the python code supposed to execute that is provided to the user proxy agent and it executes that code and as a result we get our image since the agent has the ability to find papers on archive so here I was experimenting with another prom so in the same conversation I said find papers related to flash attention and and it's able to actually pull up the papers related to flash attention now the first one is related to llms the second one doesn't seem to have anything to do with llms it seems to be more of a a material science related paper the third one also seems to be related to NLP now we can look at the internal interaction of the uh agents that we created so user proxy agent receives our uh input query the primary assistant says to find papers related to flash attention I'll use the search\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mWrite a concise summary of the following:\n",
      "\n",
      "\n",
      "\"archive function from the skills. piy so this is another skill that we give to the primary assistant and this function will uh search the archive uh database for papers that m match the query flash attention after that it writes the code it's supposed to execute in order to get uh search results so here are the search results that the uh user proxy agent got and those are then uh followed to our primary assistant to generate the final response now as you can see both the agents are working collaboratively uh to generate responses or achieve certain tasks that we are defining okay now uh let's look at the last section which is Gallery so here you can save different workflows agents and skills that you want to share now the way this works is you can simply click on publish and then you're going to see that workflow is saved in here that you can share with other people anyways this was a quick overview of autogen Studio we looked at how to set it up on your local machine we looked at different functionalities that autogen studio offers and we looked at a couple of examples I will be creating more content on this and we're going to look at some more complex workflows I hope you found this video useful if you did consider liking the video and subscribe to the channel thanks for watching and as always see you in the next one\"\n",
      "\n",
      "\n",
      "CONCISE SUMMARY:\u001b[0m\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "OpenAIChat currently only supports single prompt, got ['Write a concise summary of the following:\\n\\n\\n\"if you are building AI apps you have to learn how to use hugging face it is one of the top AI companies valued more than 2 billion dollars it has more than 16 000 followers on GitHub its product is used by Google Amazon Microsoft and meta with more than 200 000 different type of AI models including image to text text to speech below text to image and many more that\\'s why if you are building AI apps you absolutely need to learn how to use it and I\\'m going to show you how can you use hugging face platform and build it with other Public Library like launching let\\'s get to it in the show hugging face is a place for you to discover and share AI models so there are three parts of the hugging phase platform models datasets and space firstly is models this place where you can find all different sorts of models to use for example if we are interested in using image text I can select the category on the left and then on the right side it choose any of the popular image to text model and once I get into this page I on the left side they will have some description about model and on the right side it allows you to preview and test the AI model directly on their hosted version and this is why Hackney face is so useful so without it you will need to find the models download it to your local machine or host somewhere and then try to run it to know if it is the right model for you but with hugging face they are hosting on their own machine and you can test it immediately but for this image to text model I can drag and drop image directly and see what kind of results it will get if and want to use it it allows you to easily deploy this model on different servers you can also use host API on hugging face hub for free it is a bit slow and they have rate limits but it\\'s definitely enough for you to run some tests but on the other hand if you prefer to run the models locally on your own machine you can also use their Transformers library and I will talk about how do we do that very soon\"\\n\\n\\nCONCISE SUMMARY:', 'Write a concise summary of the following:\\n\\n\\n\"the listen model on the outside they also have data sets and this is where you can find a lot of data sets that you can use to train your own model for example if I want to build my own voice model I can filter down to text to speech and find a specific language that I want to use then you can click on any of them and preview what are the data sets they have unless you are training your own model you probably won\\'t use this data sets too much and the last part is space space is initially designed for people to Showcase and share the AI apps that they build so they allow you to deploy the apps that you have been building very easily on their own machine and they provide free version too but on the other side you can explore what other AI apps that people are building and there are a lot of very cool stuff you can just click on them and start playing with those apps and you can also learn how do they build those things clicking on this button it will show you all the models are used to build these apps and you can click on the files to say the source code how are we going to use those models on hugging face while we are implementing Inland chain I\\'ll take you through a step-by-step example of implementing such AI app where I can upload the image and then it can automatically turn it into an audio story the man and woman sat on the couch lost in silence he broke it I love you she smiled and said I know as through this example you will learn how to use a few different hugging face AI models let\\'s get to it firstly let\\'s think step by step how we\\'re going to implement this this app will have three components where first they need an image to text model to let the machine understand what is the scenario based on the photo and then we will use large language model to generate a short story and in the end we\\'ll use a text to speech model to generate the audio story and to find the right image to text model we can go to hugging face and filter down the image text models the\"\\n\\n\\nCONCISE SUMMARY:', 'Write a concise summary of the following:\\n\\n\\n\"one I will be using is the one called blip you will need to create a hugging face account and then go to settings access tokens and create an access token for land chain let\\'s go back to the visual studio and create a DOT NV file where we restore all the credentials an output hugging face Hub API token once I save that let\\'s import a few libraries we\\'ll reverse the import.env and run this to be able to access hugging face API token that we store in the EMV file and then we will import pipelines from Transformers so pipeline will allow us to download the hugging phase model into our local machine and now we\\'re ready to implement the first one image into text model we\\'ve created pipeline to load the AI model firstly we were putting the task here which is image of the text some of you might be curious where do we get this task name image to text from so hugging phase Transformers Library actually have a predefined list of tasks and you can go to this URL huggingface.com tasks to understand what are the tasks that it supports and you can click on any of them to get a more detailed tutorial about how to use that specific tasks and then we need to put the model name you can get a model Name by clicking on this using Transformers and just copy paste this one and we will run image to text pass on the URL of the image file then we\\'re going to print it in our copy paste a photo in the root folder for the testing purpose now let\\'s see what results we got we run python app.py now gather these results a group of people standing on a boat which is very accurate description and I only want to return the actual text here so I will do choose the library of the first item and generated tax and next is that we want to use a large language model to generate a short story based on this scenario that we got from the image you can use some open source model hugging face as well but for me I do prefer use GPT so use launching here so that\\'s adding the open AI API key here as well and then\"\\n\\n\\nCONCISE SUMMARY:', 'Write a concise summary of the following:\\n\\n\\n\"let\\'s import a few libraries from Land chain and this is a function we\\'re going to run we will firstly create a prompt template that lets GPT to generate story and then we\\'ll create this lrm chain with GPD 3.5 turbo let\\'s try and now all we need to do is just turn those tags into a speech using a text to speech model and again we will do the same thing go to the models page find text-to-speech models and find the most popular one but this time I want to share another way you can use hugging face model so you can click on this deploy button and there should be option called info difference API and this is super easy and fast way for you to test out the honeyface API for free so this is what we\\'re gonna do we\\'re gonna use this ring request Library going back here import requests and then create a function text to speech and at the top I\\'m going to load the hugging face API token so that I can pass on to the API request and I were adding the API URL here putting the header that passed on my hugging face API token and then create a message of inputs now just call this API requests and for the model I\\'m using the result it returns is a Flac file which is one type of audio file so I\\'m going to store it locally let\\'s try it oh sorry I forgot to add the OS Library which allow us to get the API token and let me try again okay here we go we got this audio file generated the group of people were standing on the boat their eyes fixed on the horizon the sample setting painting the sky with brilliant shades of pink pink and orange okay now you can see whole thing has been working all we need to do now just connecting everything together and give the UI a layer with streamlit so our import streamlit as ST which is the library that allow us create a user interface for python code and button I will add this and also create a main function which will be called when the app is loaded so first I will reset the page title and then it will give header turn image into audio story I will\"\\n\\n\\nCONCISE SUMMARY:', 'Write a concise summary of the following:\\n\\n\\n\"put a uploaded file equal to St dot file uploader this will allow people to upload a image file then if the file is uploaded I will firstly try to save this image and display the image by sd.image then I will call in the function that we created get the model to generate text from the image we uploaded and then let GPT generate a story based on the scenario in the end our general audio file from the stories and we\\'re going to display scenario and stories in here and in the end we\\'re gonna display the audio file that we got and that\\'s pretty much it let\\'s try to run this app by doing streamlit round app.p1 okay so here we can upload the image and we can see it is running here if we open the terminal we should be able to see what it\\'s doing now okay here you go you can see we already generate scenario and little story here if you click this play button it should play the audio file as they said together on the couch the man stared intently at the woman he had known for 40 years and every time he looked at her he felt like he was seeing her for the first time suddenly he blurted I think I love you the woman turned to him her eyes wide with surprise and then with a smile that lit up the room she said finally from that moment on they knew they were meant to be together okay I think of this pretty dope use case so this is how you can use hugging face models he quickly recap the easiest way for you to use that would be you can use inference API to use their hosted version directly on the other side you can also use pipeline to download all those models on your local machine if you want to learn more I highly recommend you go to thehf.com tasks to learn all the different type of tasks if it supports as well as different type of models it has one last thing I want to touch base on is I realized one of the low code AI chin build a platform called relevance AI they actually provide image to hacks model out of box and I can just create this image to speech app super quickly\"\\n\\n\\nCONCISE SUMMARY:', 'Write a concise summary of the following:\\n\\n\\n\"with their local UI and got it Droid app out of box in just like five minutes I do hope they can build a deep integration with hugging face where I can just grab different type of AI models directly but it\\'s already a pretty good start so highly recommend alright hopefully you know how to use hacking face now and start building some super interesting AI apps if you found this content useful Please Subscribe or continue sharing all those AI experiments I\\'m doing thank you and see you next time\"\\n\\n\\nCONCISE SUMMARY:', 'Write a concise summary of the following:\\n\\n\\n\"autogen from Microsoft now has a pretty nice user interface called autogen Studio you can use this user interface to create skills agents and workflows for building multi-agent capabilities autogen studio is op source and you can run this locally on your own machine and that\\'s exactly what we\\'re going to be doing today in this video I\\'ll show you how to set this up locally on your own machine and then we will look at a few examples in order to install autogen on your local machine you have two options first is to install the python package via pip and the second option is to build it from Source in this video we\\'re going to go with the easier option so we will install the python package for that first we need to create a condo virtual environment I\\'m calling the virtual environment UT to gen and we\\'re going to be using python 3.10 first just create the virtual in environment then we need to activate the virtual environment using cond activate and provide the virtual environment name you can see that the virtu environment has been activated now we need to set up our openi API key so here I\\'m providing my open a API key I will reok this after this video you can also use open source LMS with autogen Studio I\\'ll show you that in another video next we need to install the autogen studio package so we\\'re going to use pip install autogen Studio that will install the package for us you can see that it is already installed in my local machine next in order to run the autogen studio we\\'re going to use the autogen studio command with UI and we will need to run this on a specific port in this case we are running this on 8081 but you can use any port that you want when we run this this will show us a URL this is running on Local Host Port 8081 you need to copy this and paste it in your browser and here we have our autogen studio running there are three high level sections the first one is called build the second one is playground and the third one is called Gallery let\\'s go with\"\\n\\n\\nCONCISE SUMMARY:', 'Write a concise summary of the following:\\n\\n\\n\"them one by one the first section is build that focuses on defining the properties of agents and agent workflows this uh section contains the following three concepts the first one is skills second is agents and the third one is workflows let\\'s look at them one by one the first one is skills these are basically python functions that describe how the agent can solve a given task so here are a few examples they are basically python functions now each skill is supposed to have a name of the function then a descriptive doc string which describes how exactly it\\'s supposed to perform a specific task and this description is going to be used by the agents when they are introduced as part of the workflows at the inference time all these skills are going to be available to the agents you can also create your own new skills so click on new skill then provide the name of the skill that you want and here you need to replace this code uh with your own implementation the second concept is agents this provides an interface to declaratively specify property of autogen Agents so it\\'s very similar to what you probably have seen in their python code now each agent is supposed to have a name so you can provide a name here then description of the agent what exactly this agent is supposed to do you also need to provide a system message which will control the behavior of a given agent after that you can specify which different models or llms this agent can use so here the default list has GPT 4 GPT 3.5 turbo and some open source models as well you can add and remove models here and the last thing is that you need to specify which skills are available to this model so for the time being the find paper from archive X as well as generate images these are the two skills that are defined by default and you can also add new skills if you want so in this case I\\'m going to just use the gp4 preview so I will remove these other two models and then click okay so by default there are two agents one\"\\n\\n\\nCONCISE SUMMARY:', 'Write a concise summary of the following:\\n\\n\\n\"is the primary assistant the other one is user proxy so user proxy agent that execute codes if you look at it this one doesn\\'t really have any system message but you can add one here and you can also add skills now similar to skills you can also Define your own agents and you\\'ll need to provide the name description whether it can accept human input or not and the most important part is system message plus the skills that you want the agent to have in another video we\\'re going to look at how to build example agents the last section is workflows an agent workflow is a specification of a set of agents that can work together to accomplish a task so here they have provided three different example workflows let\\'s look at this General agent workflow now if you click on it first you need to provide the name of the workflow then the description of what the workflow is supposed to do so this is used for general purpose tasks then the uh next section is summary method so Define the method to summarize the conversation in agentic Frameworks like autogen multiple agents are interacting with each other to accomplish a task right so you can summarize their conversation for example this is just the last conversation if you don\\'t want any summary you set it to none or if you want the llm to generate summary of all the conversation you can send that as well now in this case the sender is going to be the user proxy agent that simply execute code and the receiver is the primary assistant that we Define in here you can add more agents in here depending on the complexity of the task that you want to achieve just like skills and agents you can also create your work workflows here the next section is playground so here you can test different workflows that you have created so for example we can create a new session let\\'s click on this next you need to select the workflow that you want to use so we will just use the general agent workflow click create and now you can start interacting with\"\\n\\n\\nCONCISE SUMMARY:', 'Write a concise summary of the following:\\n\\n\\n\"this workflow if you go back to skills so we had the generate images skill and find papers from archive so let\\'s test this agent my message is going to be create an image of an astronaut on a horse and let\\'s see what happens okay pretty cool we have an image of an astronaut on a horse and I think it was using Del to create these images now let\\'s look at what exactly happened in here so here is the python code that it created to execute our request so first it created uh a prompt or description based on our initial prompt and then it used the generate image skill to generate the image and save it you can see the actual interactions between the agents so there are a total of four different messages that were exchanged here\\'s the initial prompt so this is basically the user proxy agent that accepts the user prompt then the primary assistant agent in this case says to create an image of an astronaut on a horse I\\'ll use the generate and save images function from the skills. py file this function will generate the image based on the provided description and save it to disk let\\'s proceed proed with generating the image and here\\'s the python code it used here\\'s the file name for the python code supposed to execute that is provided to the user proxy agent and it executes that code and as a result we get our image since the agent has the ability to find papers on archive so here I was experimenting with another prom so in the same conversation I said find papers related to flash attention and and it\\'s able to actually pull up the papers related to flash attention now the first one is related to llms the second one doesn\\'t seem to have anything to do with llms it seems to be more of a a material science related paper the third one also seems to be related to NLP now we can look at the internal interaction of the uh agents that we created so user proxy agent receives our uh input query the primary assistant says to find papers related to flash attention I\\'ll use the search\"\\n\\n\\nCONCISE SUMMARY:', 'Write a concise summary of the following:\\n\\n\\n\"archive function from the skills. piy so this is another skill that we give to the primary assistant and this function will uh search the archive uh database for papers that m match the query flash attention after that it writes the code it\\'s supposed to execute in order to get uh search results so here are the search results that the uh user proxy agent got and those are then uh followed to our primary assistant to generate the final response now as you can see both the agents are working collaboratively uh to generate responses or achieve certain tasks that we are defining okay now uh let\\'s look at the last section which is Gallery so here you can save different workflows agents and skills that you want to share now the way this works is you can simply click on publish and then you\\'re going to see that workflow is saved in here that you can share with other people anyways this was a quick overview of autogen Studio we looked at how to set it up on your local machine we looked at different functionalities that autogen studio offers and we looked at a couple of examples I will be creating more content on this and we\\'re going to look at some more complex workflows I hope you found this video useful if you did consider liking the video and subscribe to the channel thanks for watching and as always see you in the next one\"\\n\\n\\nCONCISE SUMMARY:']",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m chain \u001b[38;5;241m=\u001b[39m load_summarize_chain(llm, chain_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmap_reduce\u001b[39m\u001b[38;5;124m\"\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mchain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtexts\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/langchain/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:145\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     emit_warning()\n\u001b[0;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/langchain/lib/python3.9/site-packages/langchain/chains/base.py:538\u001b[0m, in \u001b[0;36mChain.run\u001b[0;34m(self, callbacks, tags, metadata, *args, **kwargs)\u001b[0m\n\u001b[1;32m    536\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    537\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`run` supports only one positional argument.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 538\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtags\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtags\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m[\n\u001b[1;32m    539\u001b[0m         _output_key\n\u001b[1;32m    540\u001b[0m     ]\n\u001b[1;32m    542\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m kwargs \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m args:\n\u001b[1;32m    543\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m(kwargs, callbacks\u001b[38;5;241m=\u001b[39mcallbacks, tags\u001b[38;5;241m=\u001b[39mtags, metadata\u001b[38;5;241m=\u001b[39mmetadata)[\n\u001b[1;32m    544\u001b[0m         _output_key\n\u001b[1;32m    545\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/langchain/lib/python3.9/site-packages/langchain_core/_api/deprecation.py:145\u001b[0m, in \u001b[0;36mdeprecated.<locals>.deprecate.<locals>.warning_emitting_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    143\u001b[0m     warned \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    144\u001b[0m     emit_warning()\n\u001b[0;32m--> 145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mwrapped\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/langchain/lib/python3.9/site-packages/langchain/chains/base.py:363\u001b[0m, in \u001b[0;36mChain.__call__\u001b[0;34m(self, inputs, return_only_outputs, callbacks, tags, metadata, run_name, include_run_info)\u001b[0m\n\u001b[1;32m    331\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Execute the chain.\u001b[39;00m\n\u001b[1;32m    332\u001b[0m \n\u001b[1;32m    333\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    354\u001b[0m \u001b[38;5;124;03m        `Chain.output_keys`.\u001b[39;00m\n\u001b[1;32m    355\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    356\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    357\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks,\n\u001b[1;32m    358\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtags\u001b[39m\u001b[38;5;124m\"\u001b[39m: tags,\n\u001b[1;32m    359\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m: metadata,\n\u001b[1;32m    360\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrun_name\u001b[39m\u001b[38;5;124m\"\u001b[39m: run_name,\n\u001b[1;32m    361\u001b[0m }\n\u001b[0;32m--> 363\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minvoke\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcast\u001b[49m\u001b[43m(\u001b[49m\u001b[43mRunnableConfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_only_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[43minclude_run_info\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minclude_run_info\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/langchain/lib/python3.9/site-packages/langchain/chains/base.py:162\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    161\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 162\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    163\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end(outputs)\n\u001b[1;32m    164\u001b[0m final_outputs: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprep_outputs(\n\u001b[1;32m    165\u001b[0m     inputs, outputs, return_only_outputs\n\u001b[1;32m    166\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/langchain/lib/python3.9/site-packages/langchain/chains/base.py:156\u001b[0m, in \u001b[0;36mChain.invoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    149\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[1;32m    150\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    151\u001b[0m     inputs,\n\u001b[1;32m    152\u001b[0m     name\u001b[38;5;241m=\u001b[39mrun_name,\n\u001b[1;32m    153\u001b[0m )\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 156\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    157\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    158\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call(inputs)\n\u001b[1;32m    159\u001b[0m     )\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    161\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/langchain/lib/python3.9/site-packages/langchain/chains/combine_documents/base.py:136\u001b[0m, in \u001b[0;36mBaseCombineDocumentsChain._call\u001b[0;34m(self, inputs, run_manager)\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;66;03m# Other keys are assumed to be needed for LLM prediction\u001b[39;00m\n\u001b[1;32m    135\u001b[0m other_keys \u001b[38;5;241m=\u001b[39m {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m inputs\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_key}\n\u001b[0;32m--> 136\u001b[0m output, extra_return_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcombine_docs\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    137\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_run_manager\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_child\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mother_keys\u001b[49m\n\u001b[1;32m    138\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m extra_return_dict[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput_key] \u001b[38;5;241m=\u001b[39m output\n\u001b[1;32m    140\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m extra_return_dict\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/langchain/lib/python3.9/site-packages/langchain/chains/combine_documents/map_reduce.py:225\u001b[0m, in \u001b[0;36mMapReduceDocumentsChain.combine_docs\u001b[0;34m(self, docs, token_max, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcombine_docs\u001b[39m(\n\u001b[1;32m    214\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    215\u001b[0m     docs: List[Document],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    219\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[\u001b[38;5;28mstr\u001b[39m, \u001b[38;5;28mdict\u001b[39m]:\n\u001b[1;32m    220\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Combine documents in a map reduce manner.\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \n\u001b[1;32m    222\u001b[0m \u001b[38;5;124;03m    Combine by mapping first chain over all documents, then reducing the results.\u001b[39;00m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;124;03m    This reducing can be done recursively if needed (if there are many documents).\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 225\u001b[0m     map_results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_chain\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# FYI - this is parallelized and so it is fast.\u001b[39;49;00m\n\u001b[1;32m    227\u001b[0m \u001b[43m        \u001b[49m\u001b[43m[\u001b[49m\u001b[43m{\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdocument_variable_name\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpage_content\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m}\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43md\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdocs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    228\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    229\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    230\u001b[0m     question_result_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_chain\u001b[38;5;241m.\u001b[39moutput_key\n\u001b[1;32m    231\u001b[0m     result_docs \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    232\u001b[0m         Document(page_content\u001b[38;5;241m=\u001b[39mr[question_result_key], metadata\u001b[38;5;241m=\u001b[39mdocs[i]\u001b[38;5;241m.\u001b[39mmetadata)\n\u001b[1;32m    233\u001b[0m         \u001b[38;5;66;03m# This uses metadata from the docs, and the textual results from `results`\u001b[39;00m\n\u001b[1;32m    234\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i, r \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(map_results)\n\u001b[1;32m    235\u001b[0m     ]\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/langchain/lib/python3.9/site-packages/langchain/chains/llm.py:227\u001b[0m, in \u001b[0;36mLLMChain.apply\u001b[0;34m(self, input_list, callbacks)\u001b[0m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    226\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n\u001b[0;32m--> 227\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    228\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcreate_outputs(response)\n\u001b[1;32m    229\u001b[0m run_manager\u001b[38;5;241m.\u001b[39mon_chain_end({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutputs\u001b[39m\u001b[38;5;124m\"\u001b[39m: outputs})\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/langchain/lib/python3.9/site-packages/langchain/chains/llm.py:224\u001b[0m, in \u001b[0;36mLLMChain.apply\u001b[0;34m(self, input_list, callbacks)\u001b[0m\n\u001b[1;32m    219\u001b[0m run_manager \u001b[38;5;241m=\u001b[39m callback_manager\u001b[38;5;241m.\u001b[39mon_chain_start(\n\u001b[1;32m    220\u001b[0m     dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[1;32m    221\u001b[0m     {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minput_list\u001b[39m\u001b[38;5;124m\"\u001b[39m: input_list},\n\u001b[1;32m    222\u001b[0m )\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 224\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    226\u001b[0m     run_manager\u001b[38;5;241m.\u001b[39mon_chain_error(e)\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/langchain/lib/python3.9/site-packages/langchain/chains/llm.py:115\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[0;34m(self, input_list, run_manager)\u001b[0m\n\u001b[1;32m    113\u001b[0m callbacks \u001b[38;5;241m=\u001b[39m run_manager\u001b[38;5;241m.\u001b[39mget_child() \u001b[38;5;28;01mif\u001b[39;00m run_manager \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    114\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm, BaseLanguageModel):\n\u001b[0;32m--> 115\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_prompt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mllm_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    122\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm\u001b[38;5;241m.\u001b[39mbind(stop\u001b[38;5;241m=\u001b[39mstop, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_kwargs)\u001b[38;5;241m.\u001b[39mbatch(\n\u001b[1;32m    123\u001b[0m         cast(List, prompts), {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcallbacks\u001b[39m\u001b[38;5;124m\"\u001b[39m: callbacks}\n\u001b[1;32m    124\u001b[0m     )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/langchain/lib/python3.9/site-packages/langchain_core/language_models/llms.py:568\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[0;34m(self, prompts, stop, callbacks, **kwargs)\u001b[0m\n\u001b[1;32m    560\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_prompt\u001b[39m(\n\u001b[1;32m    561\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    562\u001b[0m     prompts: List[PromptValue],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    565\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    566\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    567\u001b[0m     prompt_strings \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mto_string() \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m prompts]\n\u001b[0;32m--> 568\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompt_strings\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallbacks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/langchain/lib/python3.9/site-packages/langchain_core/language_models/llms.py:741\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[0;34m(self, prompts, stop, callbacks, tags, metadata, run_name, **kwargs)\u001b[0m\n\u001b[1;32m    725\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    726\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsked to cache, but no cache found at `langchain.cache`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    727\u001b[0m         )\n\u001b[1;32m    728\u001b[0m     run_managers \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    729\u001b[0m         callback_manager\u001b[38;5;241m.\u001b[39mon_llm_start(\n\u001b[1;32m    730\u001b[0m             dumpd(\u001b[38;5;28mself\u001b[39m),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    739\u001b[0m         )\n\u001b[1;32m    740\u001b[0m     ]\n\u001b[0;32m--> 741\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate_helper\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    742\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mnew_arg_supported\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    743\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    744\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output\n\u001b[1;32m    745\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(missing_prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/langchain/lib/python3.9/site-packages/langchain_core/language_models/llms.py:605\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    603\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n\u001b[1;32m    604\u001b[0m         run_manager\u001b[38;5;241m.\u001b[39mon_llm_error(e, response\u001b[38;5;241m=\u001b[39mLLMResult(generations\u001b[38;5;241m=\u001b[39m[]))\n\u001b[0;32m--> 605\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\n\u001b[1;32m    606\u001b[0m flattened_outputs \u001b[38;5;241m=\u001b[39m output\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m    607\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m manager, flattened_output \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(run_managers, flattened_outputs):\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/langchain/lib/python3.9/site-packages/langchain_core/language_models/llms.py:592\u001b[0m, in \u001b[0;36mBaseLLM._generate_helper\u001b[0;34m(self, prompts, stop, run_managers, new_arg_supported, **kwargs)\u001b[0m\n\u001b[1;32m    582\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_generate_helper\u001b[39m(\n\u001b[1;32m    583\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    584\u001b[0m     prompts: List[\u001b[38;5;28mstr\u001b[39m],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    588\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any,\n\u001b[1;32m    589\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m LLMResult:\n\u001b[1;32m    590\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    591\u001b[0m         output \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m--> 592\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_generate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    593\u001b[0m \u001b[43m                \u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    594\u001b[0m \u001b[43m                \u001b[49m\u001b[43mstop\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstop\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    595\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;66;43;03m# TODO: support multiple run managers\u001b[39;49;00m\n\u001b[1;32m    596\u001b[0m \u001b[43m                \u001b[49m\u001b[43mrun_manager\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun_managers\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mrun_managers\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    597\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    598\u001b[0m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    599\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m new_arg_supported\n\u001b[1;32m    600\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_generate(prompts, stop\u001b[38;5;241m=\u001b[39mstop)\n\u001b[1;32m    601\u001b[0m         )\n\u001b[1;32m    602\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    603\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m run_manager \u001b[38;5;129;01min\u001b[39;00m run_managers:\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/langchain/lib/python3.9/site-packages/langchain_community/llms/openai.py:1157\u001b[0m, in \u001b[0;36mOpenAIChat._generate\u001b[0;34m(self, prompts, stop, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m   1154\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m generation \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1155\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m LLMResult(generations\u001b[38;5;241m=\u001b[39m[[generation]])\n\u001b[0;32m-> 1157\u001b[0m messages, params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_chat_params\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprompts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstop\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1158\u001b[0m params \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs}\n\u001b[1;32m   1159\u001b[0m full_response \u001b[38;5;241m=\u001b[39m completion_with_retry(\n\u001b[1;32m   1160\u001b[0m     \u001b[38;5;28mself\u001b[39m, messages\u001b[38;5;241m=\u001b[39mmessages, run_manager\u001b[38;5;241m=\u001b[39mrun_manager, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams\n\u001b[1;32m   1161\u001b[0m )\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/langchain/lib/python3.9/site-packages/langchain_community/llms/openai.py:1086\u001b[0m, in \u001b[0;36mOpenAIChat._get_chat_params\u001b[0;34m(self, prompts, stop)\u001b[0m\n\u001b[1;32m   1082\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_chat_params\u001b[39m(\n\u001b[1;32m   1083\u001b[0m     \u001b[38;5;28mself\u001b[39m, prompts: List[\u001b[38;5;28mstr\u001b[39m], stop: Optional[List[\u001b[38;5;28mstr\u001b[39m]] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple:\n\u001b[1;32m   1085\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(prompts) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m-> 1086\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1087\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOpenAIChat currently only supports single prompt, got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mprompts\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1088\u001b[0m         )\n\u001b[1;32m   1089\u001b[0m     messages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprefix_messages \u001b[38;5;241m+\u001b[39m [{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: prompts[\u001b[38;5;241m0\u001b[39m]}]\n\u001b[1;32m   1090\u001b[0m     params: Dict[\u001b[38;5;28mstr\u001b[39m, Any] \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodel\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_name}, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_default_params}\n",
      "\u001b[0;31mValueError\u001b[0m: OpenAIChat currently only supports single prompt, got ['Write a concise summary of the following:\\n\\n\\n\"if you are building AI apps you have to learn how to use hugging face it is one of the top AI companies valued more than 2 billion dollars it has more than 16 000 followers on GitHub its product is used by Google Amazon Microsoft and meta with more than 200 000 different type of AI models including image to text text to speech below text to image and many more that\\'s why if you are building AI apps you absolutely need to learn how to use it and I\\'m going to show you how can you use hugging face platform and build it with other Public Library like launching let\\'s get to it in the show hugging face is a place for you to discover and share AI models so there are three parts of the hugging phase platform models datasets and space firstly is models this place where you can find all different sorts of models to use for example if we are interested in using image text I can select the category on the left and then on the right side it choose any of the popular image to text model and once I get into this page I on the left side they will have some description about model and on the right side it allows you to preview and test the AI model directly on their hosted version and this is why Hackney face is so useful so without it you will need to find the models download it to your local machine or host somewhere and then try to run it to know if it is the right model for you but with hugging face they are hosting on their own machine and you can test it immediately but for this image to text model I can drag and drop image directly and see what kind of results it will get if and want to use it it allows you to easily deploy this model on different servers you can also use host API on hugging face hub for free it is a bit slow and they have rate limits but it\\'s definitely enough for you to run some tests but on the other hand if you prefer to run the models locally on your own machine you can also use their Transformers library and I will talk about how do we do that very soon\"\\n\\n\\nCONCISE SUMMARY:', 'Write a concise summary of the following:\\n\\n\\n\"the listen model on the outside they also have data sets and this is where you can find a lot of data sets that you can use to train your own model for example if I want to build my own voice model I can filter down to text to speech and find a specific language that I want to use then you can click on any of them and preview what are the data sets they have unless you are training your own model you probably won\\'t use this data sets too much and the last part is space space is initially designed for people to Showcase and share the AI apps that they build so they allow you to deploy the apps that you have been building very easily on their own machine and they provide free version too but on the other side you can explore what other AI apps that people are building and there are a lot of very cool stuff you can just click on them and start playing with those apps and you can also learn how do they build those things clicking on this button it will show you all the models are used to build these apps and you can click on the files to say the source code how are we going to use those models on hugging face while we are implementing Inland chain I\\'ll take you through a step-by-step example of implementing such AI app where I can upload the image and then it can automatically turn it into an audio story the man and woman sat on the couch lost in silence he broke it I love you she smiled and said I know as through this example you will learn how to use a few different hugging face AI models let\\'s get to it firstly let\\'s think step by step how we\\'re going to implement this this app will have three components where first they need an image to text model to let the machine understand what is the scenario based on the photo and then we will use large language model to generate a short story and in the end we\\'ll use a text to speech model to generate the audio story and to find the right image to text model we can go to hugging face and filter down the image text models the\"\\n\\n\\nCONCISE SUMMARY:', 'Write a concise summary of the following:\\n\\n\\n\"one I will be using is the one called blip you will need to create a hugging face account and then go to settings access tokens and create an access token for land chain let\\'s go back to the visual studio and create a DOT NV file where we restore all the credentials an output hugging face Hub API token once I save that let\\'s import a few libraries we\\'ll reverse the import.env and run this to be able to access hugging face API token that we store in the EMV file and then we will import pipelines from Transformers so pipeline will allow us to download the hugging phase model into our local machine and now we\\'re ready to implement the first one image into text model we\\'ve created pipeline to load the AI model firstly we were putting the task here which is image of the text some of you might be curious where do we get this task name image to text from so hugging phase Transformers Library actually have a predefined list of tasks and you can go to this URL huggingface.com tasks to understand what are the tasks that it supports and you can click on any of them to get a more detailed tutorial about how to use that specific tasks and then we need to put the model name you can get a model Name by clicking on this using Transformers and just copy paste this one and we will run image to text pass on the URL of the image file then we\\'re going to print it in our copy paste a photo in the root folder for the testing purpose now let\\'s see what results we got we run python app.py now gather these results a group of people standing on a boat which is very accurate description and I only want to return the actual text here so I will do choose the library of the first item and generated tax and next is that we want to use a large language model to generate a short story based on this scenario that we got from the image you can use some open source model hugging face as well but for me I do prefer use GPT so use launching here so that\\'s adding the open AI API key here as well and then\"\\n\\n\\nCONCISE SUMMARY:', 'Write a concise summary of the following:\\n\\n\\n\"let\\'s import a few libraries from Land chain and this is a function we\\'re going to run we will firstly create a prompt template that lets GPT to generate story and then we\\'ll create this lrm chain with GPD 3.5 turbo let\\'s try and now all we need to do is just turn those tags into a speech using a text to speech model and again we will do the same thing go to the models page find text-to-speech models and find the most popular one but this time I want to share another way you can use hugging face model so you can click on this deploy button and there should be option called info difference API and this is super easy and fast way for you to test out the honeyface API for free so this is what we\\'re gonna do we\\'re gonna use this ring request Library going back here import requests and then create a function text to speech and at the top I\\'m going to load the hugging face API token so that I can pass on to the API request and I were adding the API URL here putting the header that passed on my hugging face API token and then create a message of inputs now just call this API requests and for the model I\\'m using the result it returns is a Flac file which is one type of audio file so I\\'m going to store it locally let\\'s try it oh sorry I forgot to add the OS Library which allow us to get the API token and let me try again okay here we go we got this audio file generated the group of people were standing on the boat their eyes fixed on the horizon the sample setting painting the sky with brilliant shades of pink pink and orange okay now you can see whole thing has been working all we need to do now just connecting everything together and give the UI a layer with streamlit so our import streamlit as ST which is the library that allow us create a user interface for python code and button I will add this and also create a main function which will be called when the app is loaded so first I will reset the page title and then it will give header turn image into audio story I will\"\\n\\n\\nCONCISE SUMMARY:', 'Write a concise summary of the following:\\n\\n\\n\"put a uploaded file equal to St dot file uploader this will allow people to upload a image file then if the file is uploaded I will firstly try to save this image and display the image by sd.image then I will call in the function that we created get the model to generate text from the image we uploaded and then let GPT generate a story based on the scenario in the end our general audio file from the stories and we\\'re going to display scenario and stories in here and in the end we\\'re gonna display the audio file that we got and that\\'s pretty much it let\\'s try to run this app by doing streamlit round app.p1 okay so here we can upload the image and we can see it is running here if we open the terminal we should be able to see what it\\'s doing now okay here you go you can see we already generate scenario and little story here if you click this play button it should play the audio file as they said together on the couch the man stared intently at the woman he had known for 40 years and every time he looked at her he felt like he was seeing her for the first time suddenly he blurted I think I love you the woman turned to him her eyes wide with surprise and then with a smile that lit up the room she said finally from that moment on they knew they were meant to be together okay I think of this pretty dope use case so this is how you can use hugging face models he quickly recap the easiest way for you to use that would be you can use inference API to use their hosted version directly on the other side you can also use pipeline to download all those models on your local machine if you want to learn more I highly recommend you go to thehf.com tasks to learn all the different type of tasks if it supports as well as different type of models it has one last thing I want to touch base on is I realized one of the low code AI chin build a platform called relevance AI they actually provide image to hacks model out of box and I can just create this image to speech app super quickly\"\\n\\n\\nCONCISE SUMMARY:', 'Write a concise summary of the following:\\n\\n\\n\"with their local UI and got it Droid app out of box in just like five minutes I do hope they can build a deep integration with hugging face where I can just grab different type of AI models directly but it\\'s already a pretty good start so highly recommend alright hopefully you know how to use hacking face now and start building some super interesting AI apps if you found this content useful Please Subscribe or continue sharing all those AI experiments I\\'m doing thank you and see you next time\"\\n\\n\\nCONCISE SUMMARY:', 'Write a concise summary of the following:\\n\\n\\n\"autogen from Microsoft now has a pretty nice user interface called autogen Studio you can use this user interface to create skills agents and workflows for building multi-agent capabilities autogen studio is op source and you can run this locally on your own machine and that\\'s exactly what we\\'re going to be doing today in this video I\\'ll show you how to set this up locally on your own machine and then we will look at a few examples in order to install autogen on your local machine you have two options first is to install the python package via pip and the second option is to build it from Source in this video we\\'re going to go with the easier option so we will install the python package for that first we need to create a condo virtual environment I\\'m calling the virtual environment UT to gen and we\\'re going to be using python 3.10 first just create the virtual in environment then we need to activate the virtual environment using cond activate and provide the virtual environment name you can see that the virtu environment has been activated now we need to set up our openi API key so here I\\'m providing my open a API key I will reok this after this video you can also use open source LMS with autogen Studio I\\'ll show you that in another video next we need to install the autogen studio package so we\\'re going to use pip install autogen Studio that will install the package for us you can see that it is already installed in my local machine next in order to run the autogen studio we\\'re going to use the autogen studio command with UI and we will need to run this on a specific port in this case we are running this on 8081 but you can use any port that you want when we run this this will show us a URL this is running on Local Host Port 8081 you need to copy this and paste it in your browser and here we have our autogen studio running there are three high level sections the first one is called build the second one is playground and the third one is called Gallery let\\'s go with\"\\n\\n\\nCONCISE SUMMARY:', 'Write a concise summary of the following:\\n\\n\\n\"them one by one the first section is build that focuses on defining the properties of agents and agent workflows this uh section contains the following three concepts the first one is skills second is agents and the third one is workflows let\\'s look at them one by one the first one is skills these are basically python functions that describe how the agent can solve a given task so here are a few examples they are basically python functions now each skill is supposed to have a name of the function then a descriptive doc string which describes how exactly it\\'s supposed to perform a specific task and this description is going to be used by the agents when they are introduced as part of the workflows at the inference time all these skills are going to be available to the agents you can also create your own new skills so click on new skill then provide the name of the skill that you want and here you need to replace this code uh with your own implementation the second concept is agents this provides an interface to declaratively specify property of autogen Agents so it\\'s very similar to what you probably have seen in their python code now each agent is supposed to have a name so you can provide a name here then description of the agent what exactly this agent is supposed to do you also need to provide a system message which will control the behavior of a given agent after that you can specify which different models or llms this agent can use so here the default list has GPT 4 GPT 3.5 turbo and some open source models as well you can add and remove models here and the last thing is that you need to specify which skills are available to this model so for the time being the find paper from archive X as well as generate images these are the two skills that are defined by default and you can also add new skills if you want so in this case I\\'m going to just use the gp4 preview so I will remove these other two models and then click okay so by default there are two agents one\"\\n\\n\\nCONCISE SUMMARY:', 'Write a concise summary of the following:\\n\\n\\n\"is the primary assistant the other one is user proxy so user proxy agent that execute codes if you look at it this one doesn\\'t really have any system message but you can add one here and you can also add skills now similar to skills you can also Define your own agents and you\\'ll need to provide the name description whether it can accept human input or not and the most important part is system message plus the skills that you want the agent to have in another video we\\'re going to look at how to build example agents the last section is workflows an agent workflow is a specification of a set of agents that can work together to accomplish a task so here they have provided three different example workflows let\\'s look at this General agent workflow now if you click on it first you need to provide the name of the workflow then the description of what the workflow is supposed to do so this is used for general purpose tasks then the uh next section is summary method so Define the method to summarize the conversation in agentic Frameworks like autogen multiple agents are interacting with each other to accomplish a task right so you can summarize their conversation for example this is just the last conversation if you don\\'t want any summary you set it to none or if you want the llm to generate summary of all the conversation you can send that as well now in this case the sender is going to be the user proxy agent that simply execute code and the receiver is the primary assistant that we Define in here you can add more agents in here depending on the complexity of the task that you want to achieve just like skills and agents you can also create your work workflows here the next section is playground so here you can test different workflows that you have created so for example we can create a new session let\\'s click on this next you need to select the workflow that you want to use so we will just use the general agent workflow click create and now you can start interacting with\"\\n\\n\\nCONCISE SUMMARY:', 'Write a concise summary of the following:\\n\\n\\n\"this workflow if you go back to skills so we had the generate images skill and find papers from archive so let\\'s test this agent my message is going to be create an image of an astronaut on a horse and let\\'s see what happens okay pretty cool we have an image of an astronaut on a horse and I think it was using Del to create these images now let\\'s look at what exactly happened in here so here is the python code that it created to execute our request so first it created uh a prompt or description based on our initial prompt and then it used the generate image skill to generate the image and save it you can see the actual interactions between the agents so there are a total of four different messages that were exchanged here\\'s the initial prompt so this is basically the user proxy agent that accepts the user prompt then the primary assistant agent in this case says to create an image of an astronaut on a horse I\\'ll use the generate and save images function from the skills. py file this function will generate the image based on the provided description and save it to disk let\\'s proceed proed with generating the image and here\\'s the python code it used here\\'s the file name for the python code supposed to execute that is provided to the user proxy agent and it executes that code and as a result we get our image since the agent has the ability to find papers on archive so here I was experimenting with another prom so in the same conversation I said find papers related to flash attention and and it\\'s able to actually pull up the papers related to flash attention now the first one is related to llms the second one doesn\\'t seem to have anything to do with llms it seems to be more of a a material science related paper the third one also seems to be related to NLP now we can look at the internal interaction of the uh agents that we created so user proxy agent receives our uh input query the primary assistant says to find papers related to flash attention I\\'ll use the search\"\\n\\n\\nCONCISE SUMMARY:', 'Write a concise summary of the following:\\n\\n\\n\"archive function from the skills. piy so this is another skill that we give to the primary assistant and this function will uh search the archive uh database for papers that m match the query flash attention after that it writes the code it\\'s supposed to execute in order to get uh search results so here are the search results that the uh user proxy agent got and those are then uh followed to our primary assistant to generate the final response now as you can see both the agents are working collaboratively uh to generate responses or achieve certain tasks that we are defining okay now uh let\\'s look at the last section which is Gallery so here you can save different workflows agents and skills that you want to share now the way this works is you can simply click on publish and then you\\'re going to see that workflow is saved in here that you can share with other people anyways this was a quick overview of autogen Studio we looked at how to set it up on your local machine we looked at different functionalities that autogen studio offers and we looked at a couple of examples I will be creating more content on this and we\\'re going to look at some more complex workflows I hope you found this video useful if you did consider liking the video and subscribe to the channel thanks for watching and as always see you in the next one\"\\n\\n\\nCONCISE SUMMARY:']"
     ]
    }
   ],
   "source": [
    "chain = load_summarize_chain(llm, chain_type=\"map_reduce\", verbose=True)\n",
    "chain.run(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
